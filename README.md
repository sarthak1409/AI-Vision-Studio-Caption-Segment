# ğŸ§  AI Vision Studio: Caption & Segment

> **Internship Submission â€“ Zidio Development**  
> Developed by **Sarthak Maddi**
> 
## ğŸ¬ Demo Video

[![Watch the video](https://img.youtube.com/vi/DOz9BiiU-VY/maxresdefault.jpg)](https://youtu.be/DOz9BiiU-VY)

A modern **Streamlit-based AI web app** that combines **Image Captioning** and **Instance Segmentation** using state-of-the-art deep learning models.  
This project integrates **BLIP + CLIP** for intelligent caption generation and **Mask R-CNN** for precise object segmentation â€” all within a sleek, animated UI.

---

## ğŸš€ Features

- ğŸ§  **Deep Caption Generation** (BLIP + CLIP reranking)
- ğŸ¯ **Smart Image Segmentation** (Mask R-CNN pretrained on COCO)
- ğŸ–¼ï¸ **Interactive Image Comparison** (Original vs Segmented)
- ğŸ’¾ **Download Segmented Results**
- âš™ï¸ **Adjustable Parameters**
  - Segmentation confidence threshold
  - Number of caption suggestions
- ğŸŒˆ **Aurora Gradient UI** with glassmorphism styling

---

## ğŸ§© Tech Stack

| Layer | Technology |
|-------|-------------|
| **Frontend** | Streamlit |
| **Backend / Models** | PyTorch, Hugging Face Transformers |
| **Image Handling** | Pillow (PIL), OpenCV, NumPy |
| **Visualization** | Streamlit Image Comparison |
| **Deployment** | Streamlit Cloud / Localhost |

---

## ğŸ“ Folder Structure

```
internship_zidio/
â”œâ”€â”€ sample_images/               # Sample input images
â”œâ”€â”€ app.py                       # Main Streamlit app (UI + logic)
â”œâ”€â”€ caption_model.py             # BLIP + CLIP captioning module
â”œâ”€â”€ segment_model.py             # Mask R-CNN segmentation module
â”œâ”€â”€ requirements.txt             # Required dependencies
â””â”€â”€ README.md                    # Project documentation
```

---

## âš™ï¸ Installation & Setup

1. **Clone the Repository**
   ```bash
   git clone https://github.com/your-username/internship_zidio.git
   cd internship_zidio
   ```

2. **Create Virtual Environment (recommended)**
   ```bash
   python -m venv venv
   source venv/bin/activate      # (on macOS/Linux)
   venv\Scripts\activate       # (on Windows)
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the App**
   ```bash
   streamlit run app.py
   ```

5. **Access the App**
   Open your browser and visit:
   ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ğŸ§  Model Details

| Component         | Model Used                                       | Source                        |
| ----------------- | ------------------------------------------------ | ----------------------------- |
| Captioning        | **BLIP (Salesforce/blip-image-captioning-base)** | Hugging Face                  |
| Caption Reranking | **CLIP (openai/clip-vit-base-patch32)**          | Hugging Face                  |
| Segmentation      | **Mask R-CNN (ResNet-50 FPN)**                   | TorchVision Pretrained Models |

Each image uploaded passes through BLIP for caption generation and CLIP for semantic reranking.  
For segmentation, Mask R-CNN identifies objects and overlays colored masks with confidence thresholds.

---

## ğŸ’¡ How It Works

1. **Upload an Image**
2. **Step 1: Deep Captioning**
   * BLIP generates multiple candidate captions.
   * CLIP scores each caption and selects the most relevant one.
3. **Step 2: Instance Segmentation**
   * Mask R-CNN detects and labels objects.
   * Overlays masks and bounding boxes.
4. **Visual Comparison**
   * View side-by-side comparison of original and segmented images.
5. **Download Results**
   * Save your segmented image in `.png` format.

---

## ğŸ¨ UI Highlights

* Dynamic **Aurora Gradient Background**
* Smooth animations & **Glassmorphic containers**
* Clean sidebar with parameter sliders
* Footer credits and branding for Zidio Internship

---

## ğŸ“¦ Requirements

All dependencies are listed in [`requirements.txt`](requirements.txt):

```
torch
torchvision
transformers
pillow
opencv-python
matplotlib
streamlit
numpy
timm
ftfy
streamlit-image-comparison
```

---

## ğŸ“¸ Sample Demo

> Upload any image from `sample_images/` and try the caption + segmentation features.  

---

## ğŸ™Œ Credits

**Developed by:** Sarthak Maddi  
**Organization:** Zidio Development
**Year:** 2025

---

## ğŸ›¡ï¸ License

This project is open-source and available for academic and research use under the MIT License.
